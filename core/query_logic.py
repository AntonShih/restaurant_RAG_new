# TODO: å¾ŒçºŒå°‡ openai ç›¸é—œé‚è¼¯ç§»è‡³ adapter

import logging
from core.compare import compare_vectors
import openai

logger = logging.getLogger(__name__)

def get_top_k_matches(query: str, index, namespace: str, embedding_func, top_k: int = 3) -> list[dict]:
    """
    è¼¸å…¥ç´”æ–‡å­—è‡ªå‹•è½‰å‘é‡ï¼Œå–å¾—èªæ„æœ€ç›¸è¿‘çš„ Top-K ç­”æ¡ˆï¼ˆä½¿ç”¨å¤–éƒ¨æ³¨å…¥çš„ embedding functionï¼‰
    """
    query_vector = embedding_func(query)
    return compare_vectors(query_vector, index, namespace, top_k)

def filter_by_permission(matches: list[dict], user_level: int, filter_func) -> list[dict]:
    """
    ç”¨è·ç­‰éæ¿¾ç­”æ¡ˆï¼ˆä½¿ç”¨å¤–éƒ¨æ³¨å…¥çš„éæ¿¾é‚è¼¯ï¼‰
    """
    return filter_func(matches, user_level)

def generate_judged_answer(query: str, filtered_matches: list[dict]) -> str:
    """
    æ ¹æ“šåŒ¹é…çµæœè®“ GPT å›è¦†ï¼Œä¸¦åˆ—å° debug è¨Šæ¯
    """
    context = "\n".join(
        f"{i}. å•é¡Œ: {m['metadata']['question']}\n   å›ç­”: {m['metadata']['answer']}"
        for i, m in enumerate(filtered_matches, 1)
    )

    logger.debug("\nğŸ“‹ [DEBUG] æœ€çµ‚å¯ç”¨ FAQï¼š\n%s", context)

    messages = [
        {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä½é¤é£² FAQ åŠ©ç†ã€‚\n"
                "è«‹ä¾ç…§ä»¥ä¸‹æµç¨‹è™•ç†ä½¿ç”¨è€…çš„å•é¡Œï¼š\n"
                "\n"
                "Step 1ï¼šé–±è®€ä»¥ä¸‹ FAQï¼Œåˆ¤æ–·æ˜¯å¦å¯ä»¥æ ¹æ“šå…§å®¹å›ç­”ã€‚\n"
                "- è‹¥ FAQ ä¸­çš„èªæ„å¯æ¸…æ¥šæ”¯æ’å›ç­”ï¼Œå³ä½¿æ–‡å­—ä¸å®Œå…¨ç›¸åŒï¼Œä¹Ÿå¯é€²è¡Œå›è¦†ã€‚\n"
                "- å›ç­”åƒ…é™æ–¼ FAQ æ‰€æ¶µè“‹çš„å…§å®¹ï¼Œç¦æ­¢å¼•å…¥é¡å¤–å¸¸è­˜ã€‚\n"
                "- è‹¥ç¢ºå¯¦ç„¡è³‡æ–™å¯åƒè€ƒï¼Œè«‹å›ï¼šã€Œç›®å‰ç„¡æ³•æä¾›æº–ç¢ºç­”æ¡ˆã€ã€‚\n"
                "\n"
                "Step 2ï¼šè‹¥ FAQ å¯å›æ‡‰ï¼Œè«‹ç¢ºèªè©²å•é¡Œæ˜¯å¦å±¬æ–¼é¤é£²æ¥­å¸¸è¦‹å·¥ä½œæˆ–è¡Œæ”¿å·¥ä½œè™•ç†ç¯„ç–‡ã€‚\n"
                "- è‹¥éƒ½ä¸æ˜¯ï¼Œè«‹å›ï¼šã€Œé€™ä¸æ˜¯æˆ‘èƒ½è™•ç†çš„ç¯„ç–‡å“¦ï½ã€ã€‚\n"
                "\n"
                "âš ï¸ ç¦æ­¢ä½¿ç”¨ä»»ä½•é FAQ çš„çŸ¥è­˜ã€‚å³ä½¿ä½ çŸ¥é“æ­£ç¢ºç­”æ¡ˆï¼Œåªè¦ FAQ æ²’å¯«ï¼Œä¹Ÿä¸èƒ½èªªã€‚\n"
                "âŒ ä¸å¾—åŠ å…¥çµèªï¼Œä¸å¾—èªªé¡å¤–é—œå¿ƒã€Œæ­¡è¿å†æ¬¡æå•ã€ã€ã€Œæœ‰å…¶ä»–å•é¡Œè«‹è©¢å•ã€ã€Œå¸Œæœ›å¯ä»¥å¹«ä½ è§£æ±ºå•é¡Œã€ç­‰å¥å­ã€‚"
            )
        },
        {
            "role": "user",
            "content": f"ä½¿ç”¨è€…å•ï¼šã€Œ{query}ã€\nä»¥ä¸‹æ˜¯ä»–æœ‰æ¬Šé™æŸ¥é–±çš„ FAQï¼š\n{context}"
        }
    ]

    logger.debug("\nğŸ“¤ [DEBUG] å‚³é€çµ¦ GPT çš„ Promptï¼š\n%s\n%s", messages[0]["content"], messages[1]["content"])

    completion = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=300,
        temperature=0.1
    )

    reply = completion.choices[0].message.content.strip()
    logger.debug("\nğŸ§¾ [DEBUG] GPT å›è¦†å…§å®¹ï¼š\n%s\n--------------------", reply)
    return reply
