import openai
from RAG.core.compare import search_similar_faqs
from line_bot.services.user_service import get_user_role
from config.environment import init_openai,get_pinecone_index,get_namespace,init_pinecone

import logging
logger = logging.getLogger(__name__)

def answer_query_secure(query: str, user_id: str, index, namespace):
    
    """å®‰å…¨ç‰ˆæŸ¥è©¢ï¼šæŸ¥ top3ï¼Œæ¬Šé™éæ¿¾å¾Œäº¤ç”± LLM åˆ¤æ–·"""
    user = get_user_role(user_id)
    user_level = user.get("access_level", 0) if user else 0

    #æŸ¥top3æœ‰æ²’æœ‰ç¬¦åˆæ¬Šé™å¯çœ‹å…§å®¹ ç•™ä¸‹å¯çœ‹çš„
    matches = search_similar_faqs(query, index, namespace, top_k=3)
    filtered_matches = [m for m in matches if m["metadata"].get("access_level") <= user_level]

    logger.info(f"ä½¿ç”¨è€… {user_id} æŸ¥è©¢ï¼š{query}")
    logger.debug(f"åŸå§‹æ¯”å°ç­†æ•¸ï¼š{len(matches)}ï¼›æ¬Šé™å…§ç­†æ•¸ï¼š{len(filtered_matches)}")

    if not filtered_matches:
        logger.warning(f"ä½¿ç”¨è€… {user_id} ç„¡æ³•æŸ¥è©¢æ¬Šé™å…§ FAQ")
        return "âš ï¸ æ‚¨çš„è·ç­‰ç„¡æ³•æŸ¥é–±è³‡æ–™ï¼Œè«‹æ´½è©¢ç®¡ç†è€…"

    return generate_judged_answer(query, filtered_matches)


def generate_judged_answer(query, filtered_matches):
    """è«‹ LLM æ ¹æ“šéæ¿¾å¾Œçš„å…§å®¹ç”Ÿæˆå›è¦†æˆ–æ‹’ç­”
    
    âš ï¸ å›å‚³æ ¼å¼ï¼ˆopenai.chat.completions.createï¼‰ï¼š
    {
        "id": "...",
        "object": "chat.completion",
        "choices": [
            {
                "message": {
                    "role": "assistant",
                    "content": "æ‚¨å¥½ï¼Œæ­¡è¿å…‰è‡¨æœ¬é¤å»³ï¼"
                }
            }
        ],
        "usage": {
            "prompt_tokens": 100,
            "completion_tokens": 50,
            "total_tokens": 150
        }
    }
    
    """
    context = ""
    # é€™é‚Šçš„1æ˜¯enumerateå¾1è™Ÿé–‹å§‹contextåªçµ¦ä»–å•é¡Œè·Ÿç­”æ¡ˆè€Œå·²ï¼Œèªæ„å®Œæ•´
    for i, m in enumerate(filtered_matches, 1):
        meta = m["metadata"]
        context += f"{i}. å•é¡Œ: {meta['question']}\n   å›ç­”: {meta['answer']}\n"

    print("\nğŸ“‹ [DEBUG] æœ€çµ‚å¯ç”¨ FAQï¼š")
    print(context)

    messages = [
        {
            "role": "system",
            "content": (
                "ä½ æ˜¯ä¸€ä½é¤é£² FAQ åŠ©ç†ã€‚\n"
                "è«‹ä¾ç…§ä»¥ä¸‹æµç¨‹è™•ç†ä½¿ç”¨è€…çš„å•é¡Œï¼š\n"
                "\n"
                "Step 1ï¼šé–±è®€ä»¥ä¸‹ FAQï¼Œåˆ¤æ–·æ˜¯å¦å¯ä»¥æ ¹æ“šå…§å®¹å›ç­”ã€‚\n"
                "- è‹¥ FAQ ä¸­çš„èªæ„å¯æ¸…æ¥šæ”¯æ’å›ç­”ï¼Œå³ä½¿æ–‡å­—ä¸å®Œå…¨ç›¸åŒï¼Œä¹Ÿå¯é€²è¡Œå›è¦†ã€‚\n"
                "- å›ç­”åƒ…é™æ–¼ FAQ æ‰€æ¶µè“‹çš„å…§å®¹ï¼Œç¦æ­¢å¼•å…¥é¡å¤–å¸¸è­˜ã€‚\n"
                "- è‹¥ç¢ºå¯¦ç„¡è³‡æ–™å¯åƒè€ƒï¼Œè«‹å›ï¼šã€Œç›®å‰ç„¡æ³•æä¾›æº–ç¢ºç­”æ¡ˆã€ã€‚\n"
                "\n"
                "Step 2ï¼šè‹¥ FAQ å¯å›æ‡‰ï¼Œè«‹ç¢ºèªè©²å•é¡Œæ˜¯å¦å±¬æ–¼é¤é£²æ¥­å¸¸è¦‹å·¥ä½œæˆ–è¡Œæ”¿å·¥ä½œè™•ç†ç¯„ç–‡ã€‚\n"
                "- è‹¥éƒ½ä¸æ˜¯ï¼Œè«‹å›ï¼šã€Œé€™ä¸æ˜¯æˆ‘èƒ½è™•ç†çš„ç¯„ç–‡å“¦ï½ã€ã€‚\n"
                "\n"
                "âš ï¸ ç¦æ­¢ä½¿ç”¨ä»»ä½•é FAQ çš„çŸ¥è­˜ã€‚å³ä½¿ä½ çŸ¥é“æ­£ç¢ºç­”æ¡ˆï¼Œåªè¦ FAQ æ²’å¯«ï¼Œä¹Ÿä¸èƒ½èªªã€‚\n"
                "âŒ ä¸å¾—åŠ å…¥çµèªï¼Œä¸å¾—èªªé¡å¤–é—œå¿ƒã€Œæ­¡è¿å†æ¬¡æå•ã€ã€ã€Œæœ‰å…¶ä»–å•é¡Œè«‹è©¢å•ã€ã€Œå¸Œæœ›å¯ä»¥å¹«ä½ è§£æ±ºå•é¡Œã€ç­‰å¥å­ã€‚"
            )
        },
        {
            "role": "user",
            "content": f"ä½¿ç”¨è€…å•ï¼šã€Œ{query}ã€\nä»¥ä¸‹æ˜¯ä»–æœ‰æ¬Šé™æŸ¥é–±çš„ FAQï¼š\n{context}"
        }
    ]

    print("\nğŸ“¤ [DEBUG] å‚³é€çµ¦ GPT çš„ Promptï¼š")
    print(messages[0]["content"])
    print(messages[1]["content"])

    completion = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=300,
        temperature=0.1
    )

    reply = completion.choices[0].message.content.strip()
    
    print("\nğŸ§¾ [DEBUG] GPT å›è¦†å…§å®¹ï¼š")
    print(reply)
    print("--------------------\n")

    return reply

if __name__ == "__main__":
    # æ¸¬è©¦ poetry run python -m RAG.query.query_engine_safe

    init_openai()
    init_pinecone()
    index = get_pinecone_index()
    namespace = get_namespace()

    # æ‰‹å‹•æ¸¬è©¦å€ï¼šè¼¸å…¥å•é¡Œèˆ‡ä½¿ç”¨è€… ID ä¾†æ¨¡æ“¬æŸ¥è©¢æµç¨‹
    query = input("è«‹è¼¸å…¥ä½ è¦æŸ¥è©¢çš„å•é¡Œï¼š\n> ")
    user_id = input("è«‹è¼¸å…¥æ¨¡æ“¬çš„ä½¿ç”¨è€… IDï¼š\n> ")

    answer = answer_query_secure(query, user_id, index, namespace)
    
    print("\nğŸ’¬ æœ€çµ‚å›è¦†å…§å®¹ï¼š")
    print(answer)

# -----------------------------------------------------------------------
# async

# import os
# import sys
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
# from RAG.core.compare import search_similar_faqs
# from openai import AsyncOpenAI
# from pinecone import Pinecone
# from line_bot.services.user_service import get_user_role

# client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# async def answer_query_secure(query, user_id):
#     pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
#     index = pc.Index(os.getenv("PINECONE_INDEX_NAME"))
#     namespace = os.getenv("PINECONE_NAMESPACE")

#     user = get_user_role(user_id)
#     user_level = user.get("access_level", 0) if user else 0

#     matches = search_similar_faqs(query, index, namespace, top_k=3)
#     filtered_matches = [m for m in matches if m["metadata"].get("access_level") <= user_level]

#     print("ğŸ” [DEBUG] ä½¿ç”¨è€…å•é¡Œï¼š", query)
#     print("ğŸ‘¤ [DEBUG] ä½¿ç”¨è€… IDï¼š", user_id, "è·ç­‰ç­‰ç´šï¼š", user_level)

#     print("ğŸ“¥ Top3 FAQ:")
#     for m in matches:
#         meta = m["metadata"]
#         print(f" - ({meta.get('access_level')}) {meta['question']}")

#     print("ğŸ”’ Filtered FAQï¼ˆç¬¦åˆè·ç­‰ï¼‰:")
#     for m in filtered_matches:
#         meta = m["metadata"]
#         print(f" âœ… ({meta.get('access_level')}) {meta['question']}")

#     if not filtered_matches:
#         return "âš ï¸ æŠ±æ­‰ï¼Œæ‚¨ç›®å‰çš„è·ç­‰ç„¡æ³•æŸ¥é–±ç›¸é—œè³‡æ–™ï¼Œè«‹æ´½è©¢ä¸Šç´šæˆ–ç®¡ç†è€…ã€‚"

#     return await generate_judged_answer(query, filtered_matches)


# def get_embedding(query):
#     # TODO: æ”¹ç‚ºå¯¦éš›åµŒå…¥é‚è¼¯ï¼Œå¦‚ OpenAI Embedding
#     return [0.1] * 1536


# async def generate_judged_answer(query, filtered_matches):
#     context = ""
#     for i, m in enumerate(filtered_matches, 1):
#         meta = m["metadata"]
#         context += f"{i}. å•é¡Œ: {meta['question']}\n   å›ç­”: {meta['answer']}\n"

#     print("\nğŸ“‹ [DEBUG] æœ€çµ‚å¯ç”¨ FAQï¼š")
#     print(context)

#     messages = [
#         {
#             "role": "system",
#             "content": (
#                 "ä½ æ˜¯ä¸€ä½é¤é£² FAQ åŠ©ç†ã€‚\n"
#                 "è«‹ä¾ç…§ä»¥ä¸‹æµç¨‹è™•ç†ä½¿ç”¨è€…çš„å•é¡Œï¼š\n"
#                 "\n"
#                 "Step 1ï¼šé–±è®€ä»¥ä¸‹ FAQï¼Œåˆ¤æ–·æ˜¯å¦å¯ä»¥æ ¹æ“šå…§å®¹å›ç­”ã€‚\n"
#                 "- è‹¥ FAQ ä¸­çš„èªæ„å¯æ¸…æ¥šæ”¯æ’å›ç­”ï¼Œå³ä½¿æ–‡å­—ä¸å®Œå…¨ç›¸åŒï¼Œä¹Ÿå¯é€²è¡Œå›è¦†ã€‚\n"
#                 "- å›ç­”åƒ…é™æ–¼ FAQ æ‰€æ¶µè“‹çš„å…§å®¹ï¼Œç¦æ­¢å¼•å…¥é¡å¤–å¸¸è­˜ã€‚\n"
#                 "- è‹¥ç¢ºå¯¦ç„¡è³‡æ–™å¯åƒè€ƒï¼Œè«‹å›ï¼šã€Œç›®å‰ç„¡æ³•æä¾›æº–ç¢ºç­”æ¡ˆã€ã€‚\n"
#                 "\n"
#                 "Step 2ï¼šè‹¥ FAQ å¯å›æ‡‰ï¼Œè«‹ç¢ºèªè©²å•é¡Œæ˜¯å¦å±¬æ–¼é¤é£²æ¥­å¸¸è¦‹å·¥ä½œæˆ–è¡Œæ”¿å·¥ä½œè™•ç†ç¯„ç–‡ã€‚\n"
#                 "- è‹¥éƒ½ä¸æ˜¯ï¼Œè«‹å›ï¼šã€Œé€™ä¸æ˜¯æˆ‘èƒ½è™•ç†çš„ç¯„ç–‡å“¦ï½ã€ã€‚\n"
#                 "\n"
#                 "âš ï¸ ç¦æ­¢ä½¿ç”¨ä»»ä½•é FAQ çš„çŸ¥è­˜ã€‚å³ä½¿ä½ çŸ¥é“æ­£ç¢ºç­”æ¡ˆï¼Œåªè¦ FAQ æ²’å¯«ï¼Œä¹Ÿä¸èƒ½èªªã€‚\n"
#                 "âŒ ä¸å¾—åŠ å…¥çµèªï¼Œä¸å¾—èªªé¡å¤–é—œå¿ƒã€Œæ­¡è¿å†æ¬¡æå•ã€ã€ã€Œæœ‰å…¶ä»–å•é¡Œè«‹è©¢å•ã€ã€Œå¸Œæœ›å¯ä»¥å¹«ä½ è§£æ±ºå•é¡Œã€ç­‰å¥å­ã€‚"
#             )
#         },
#         {
#             "role": "user",
#             "content": f"ä½¿ç”¨è€…å•ï¼šã€Œ{query}ã€\nä»¥ä¸‹æ˜¯ä»–æœ‰æ¬Šé™æŸ¥é–±çš„ FAQï¼š\n{context}"
#         }
#     ]

#     print("\nğŸ“¤ [DEBUG] å‚³é€çµ¦ GPT çš„ Promptï¼š")
#     print(messages[0]["content"])
#     print(messages[1]["content"])

#     try:
#         completion = await client.chat.completions.create(
#             model="gpt-3.5-turbo",
#             messages=messages,
#             max_tokens=300,
#             temperature=0.1
#         )
#         reply = completion.choices[0].message.content.strip()
#     except Exception as e:
#         print("âŒ GPT å‘¼å«å¤±æ•—ï¼š", str(e))
#         return "âŒ å›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

#     print("\nğŸ§¾ [DEBUG] GPT å›è¦†å…§å®¹ï¼š")
#     print(reply)
#     print("--------------------\n")

#     return reply


# if __name__ == "__main__":
#     import asyncio

#     async def test():
#         query = input("è«‹è¼¸å…¥ä½ è¦æŸ¥è©¢çš„å•é¡Œï¼š\n> ")
#         user_id = input("è«‹è¼¸å…¥æ¨¡æ“¬çš„ä½¿ç”¨è€… IDï¼š\n> ")
#         answer = await answer_query_secure(query, user_id)
#         print("\nğŸ’¬ æœ€çµ‚å›è¦†å…§å®¹ï¼š")
#         print(answer)

#     asyncio.run(test())

